Intro

<ul><li>One of the main buildings blocks of aws</li>
<li>advertised as "infinitely scaling" storage</li>
<li>Many websites use amazon s3 as a backbone</li>
<li>many asws services use s3 as an integration as well</li>
</ul>
<h1>Amazon S3 Use cases</h1>

<ul><li>Backup and storage</li>
<li>Disaster Recovery</li>
<li>Archive</li>
<li>Hybrid Cloud storage - Application hosting</li>
<li>Media hosting</li>
<li>Data lakes & big data analytics - Software delivery</li>
<li>Static website</li>
</ul>
<h1>S3 - Buckets</h1>

<ul><li>S3 allows people to store objects(files) in "buckets" (directories)</li>
<li>Buckets MUST have a globally unique name (across all regions all accounts)</li>
<li>buckets are defined at the region level</li>
<li>S3 looks like a global service, but buckets are created in a region</li>
<li>Naming convention for buckets:</li>
</ul>  -No uppercase, No underscore
  -3-63 characters long
  -Not an IP address
  -Must start with lowercase letter or number
  -Must NOT start with the prefix <b>xn--</b>
  -Must NOT end with the suffix <b>-s3alias</b>
<ul><li>buckets have encryption configured by default, objects are automatically encrypted</li>
</ul>
<h1>S3 - Objects</h1>

<ul><li>objects are a file that have a key</li>
<li>Objects(files) have a Key</li>
<li>the key: is the FULL path:</li>
</ul>  -s3://my-bucket/my_file.txt
  -OR (if it is in a bucket)
  -s3://my-bucket/my<i>folder1/another</i>_folder/my_file.txt
<ul><li>the key is composed of a prefix + object name</li>
</ul>  -Ex:
  -s3://my-bucket/my<i>file.txt => s3://my-bucket/my</i>_folder1/ (prefix) + my_file.txt(object)
<ul><li>There's no concept of "directories" within buckets(but the UI will trick you to think otherwise)</li>
</ul>  -Just keys with long names that contain slashes "/"
<ul><li>Object values are the file content and the metadata</li>
</ul>  -Object values are the content of the body(a file)
  -Max object size is 5TB
  -if uploading more than 5 GB, must use "multi-part upload"
  -Metadata (list of text key / value pairs - system or user metadata)
  -Tags (Unicode key / value pair - up to 10) - useful for security / lifecycle
  -Version ID (if versioning is enabled)

<h1>S3 - Security</h1>

<ul><li>User-Based</li>
</ul>  -- which API calls should be allowed for a specific user from IAM
<ul><li>Resource-Based</li>
</ul>  -Bucket Policies: bucket wide rules from the S3 console -
  -ex: can allows cross account access(allowing a user from a different account to view our buckets)
  -Object Access Control List(ACL): finer grain (can be disabled)
  -Bucket Access Control List (ACL): less common(can be disabled)
<ul><li>Note: an IAM principal can access an S3 object if:</li>
</ul>  -the user's IAM permissions Allow it OR the resource policy allows it AND there's no explicit deny
<ul><li>Encryption: encrypts objects in Amazon S3 using encryption keys</li>
</ul>
<h1>S3 Bucket policies</h1>

<ul><li>JSON based policies - See: for something similar</li>
<li>Resources: buckets and objects(what to apply this policy to)</li>
<li>Effect: Allow or Deny</li>
<li>Actions: Set of API actions to allow or deny (what actions to allow or deny)</li>
<li>Principal: Account or user to apply the policy to (\* = all users and accounts)</li>
<li>Use S3 bucket for policy to:</li>
</ul>  -Grant public access to the bucket
  -Force objects to the encrypted at upload
  -Grant access to another account(Cross account)

<h3>Example situation: Public Access - Use Bucket policy</h3>

<ul><li>an anonymous visitor wants to access your bucket</li>
<li>once the bucket policy has been attached, the visitor can access the bucket</li>
</ul>
<h3>Example: User Access to S3 - IAM permissions</h3>

<ul><li>giving access to an IAM user</li>
<li>we assign an IAM policy to the IAM user. then they can access the S3 bucket</li>
</ul>
<h3>Example: EC2 Instance access - Use IAM roles</h3>

<ul><li>To allow an EC2 instance to read an S3 bucket:</li>
<li>Create an IAM role (an EC2 instance role in this case) to give permission to the EC2 instance</li>
</ul>
<h3>Advanced: Cross-account access - use bucket policy</h3>

<ul><li>if we want to allow a cross-account access from an IAM user:</li>
<li>Create an S3 bucket policy that allows cross-account access</li>
</ul>
<h3>Bucket settings for Block Public access</h3>

<ul><li>These settings were created to prevent company data leaks</li>
<li>if you know your bucket should never be public, leave these on</li>
<li>Can be set at the account level</li>
</ul>
<h1>S3 - Static Website Hosting</h1>

<ul><li>S3 can host static websites and have them accessible on the internet</li>
<li>The website URL will be:</li>
</ul>  -\http://bucket-name.s3-website-aws-region.amazonaws.com OR
  -\http://bucket-name.s3-website.aws-region.amazonaws.com
<ul><li>need to have public reads available for this to work</li>
<li>If you get a 403 forbidden error: make sure the bucket policy allows public reads</li>
</ul>
<h1>S3 - Versioning</h1>

<ul><li>you can version your files in Amazon S3</li>
<li>it is enabled at the bucket level</li>
<li>Same key overwrite will change the "version": 1,2,3</li>
<li>It is best practice to version your buckets</li>
</ul>  - why?:
  - Protect against unintended deletes(ability to restore a version)
  - Fast roll back to previous versions
<ul><li>Notes</li>
</ul>  - any file that is not versioned prior to enabling versioning will have version "null"
    -suspending version does NOT delete the previous versions

<h1>S3 - Replication(CRR & SRR)</h1>

<ul><li>Must <b>enable Versioning</b> in source and destination buckets</li>
<li>## Goal: async replication of data from bucket to another</li>
<li><b>Cross-Region Replication:</b>source and destination buckets are in different regions</li>
<li><b>Same-Region Replication</b> source and destination buckets are in different regions</li>
<li>Buckets <b>can be in different AWS accounts</b></li>
<li>Copying is asynchronous</li>
<li>Must give proper IAM permissions to S3</li>
<li>Use Cases:</li>
</ul>  - CRR - compliance, lower latency access, replication across accounts
  - SRR - log aggregation, live replication between production and test accounts

<h1>S3 Durability and Availability</h1>

<ul><li>Durability</li>
</ul>  -High durability(Eleven 9s) of objects across multiple AZs
  -Same for all storage classes
  -If you store 10,000,000 objects with Amazon S3, you can on average expect to incur a loss of a single object once every 10,000 years
<ul><li>Availability</li>
</ul>  - Measures how readily available a service is
  - Varies depending on storage class
  - Ex: S3 standard has 99.99% availability = not available 53 minutes a year

<h1>S3 Storage Classes</h1>

All Storage classes:

<ul><li>Amazon S3 Standard - General Purpose</li>
<li>Amazon S3 Standard-Infrequent access (IA)</li>
<li>Amazon S3 One Zone-Infrequent access</li>
<li>Amazon S3 Glacier Instant Retrieval</li>
<li>Amazon S3 Glacier Flexible Retrieval</li>
<li>Amazon S3 Glacier Deep Archive</li>
<li>Amazon S3 Intelligent Tiering</li>
<li>Can move between classes manually or using S3 Lifecycle configurations</li>
</ul>
<h2>S3 Standard - General Purpose</h2>

<ul><li>Used for: frequently accessed data</li>
<li>low latency, high throughput</li>
<li>use cases: big data analytics, mobile & gaming applications, content distribution, etc</li>
</ul>
<h2>S3 Storage Classe: Infrequent Access</h2>

<ul><li>Used for: data that is less frequently accessed, but requires rapid access when needed</li>
<li>Cheaper than S3 standard</li>
<li>Amazon S3 Standard-Infrequent Access (S3 Standard-IA)</li>
</ul>  - 4 9s availability
  - Use cases: Disaster recovery, backups
<ul><li>Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)</li>
</ul>  - High durability(Eleven 9s) in a single AZ, data lost when AZ is destroyed
  - 99.5% availability
  - Use cases: storing secondray backup copies of on-premise data, or data you can recreate

<h2>Amazon S3 Glacier Storage Classes</h2>

<ul><li>Low cost object storage</li>
<li>Used for: archiving/backup</li>
<li>Pricing: price for storage + object retrieval</li>
<li>Amazon S3 Glacier Instant Retrieval</li>
</ul>  -Millisecond retrieval, great for data accessed once a quarter * Minimum storage duration of 90 days
<ul><li>Amazon S3 Glacier Flexible Retrieval (formerly Amazon S3 Glacier):</li>
<li>Expedited (1 to 5 minutes), Standard (3 to 5 hours), Bulk (5 to 12 hours) - free</li>
<li>Minimum storage duration of 90 days</li>
<li>Amazon S3 Glacier Deep Archive - for long term storage:</li>
<li>Standard (12 hours), Bulk (48 hours)</li>
<li>Minimum storage duration of 180 days</li>
</ul>
<h2>S3 Intelligent-Tiering</h2>

<ul><li>function: Moves objects automatically between access tiers based on usage</li>
<li>No retrieval charges in S3 Intelligent-Tiering</li>
<li>Small monthly monitoring and auto-tiering fee</li>
<li>Frequent Access tier (automatic): default tier</li>
<li>Infrequent Access tier (automatic): objects not accessed for 30 days</li>
<li>Archive Instant Access tier (automatic): objects not accessed for 90 days</li>
<li>Archive Access tier (optional): configurable from 90 days to 700+ days</li>
<li>Deep Archive Access tier (optional): config. from 180 days to 700+ days</li>
</ul>
<h2>S3 Storage Classes Comparison</h2>

<h2>S3 Storage classes - Price Comparison example(us-east-1)</h2>

<h2>S3 Encryption</h2>

<h1>Shared Responsbility Model for S3</h1>

<ul><li>AWS: Infrastructure, Configuration, vulnerability analysis, compliance validation</li>
<li>User: S3 Versioning, S3 Bucket policies, S3 replication setup, Logging and monitoring, S3 storage classes</li>
<li>Data encryption and in transit</li>
</ul>
<h1>aws storage gateway</h1>

<h2>Hybrid Cloud for Storage</h2>

<ul><li>aws is pushing for "hybrid cloud": some infra on-prem and some infra in the cloud</li>
<li>Why hybrid?</li>
</ul>  -Long cloud migrations
  -Security requirements
  -Compliance requirements
  -IT strategy
<ul><li>Problem: S3 is a propietary storage technology(unlike EFS/NFS) so how do you expose the S3 data on-premise?</li>
<li>Solution: aws storage gateway</li>
</ul>
<h2>aws storage gateway</h2>

<ul><li>Bridge between on-premise data and cloud-data in S3</li>
<li>Hybrid storage service to allow on-premises to seamlessly use the AWS Cloud</li>
<li>defn: hybrid solution to extend on-premises storage to S3</li>
<li>Use cases: disaster recovery, backup & restore, tiered storage</li>
<li>Don't need to know types of storage gateway</li>
</ul>
<h2>AWS Storage Cloud-Native Options</h2>

<h1>S3 - Summary</h1>

<ul><li>Buckets v. Objects</li>
<li>S3 security</li>
<li>S3 websites</li>
<li>S3 versioning</li>
<li>S3 replication</li>
<li>S3 replication</li>
<li>Snow Family</li>
<li>OpsHub</li>
<li>Storage Gateway</li>
</ul>
Intro

<ul><li>One of the main buildings blocks of aws</li>
<li>advertised as "infinitely scaling" storage</li>
<li>Many websites use amazon s3 as a backbone</li>
<li>many asws services use s3 as an integration as well</li>
</ul>
<h1>Amazon S3 Use cases</h1>

<ul><li>Backup and storage</li>
<li>Disaster Recovery</li>
<li>Archive</li>
<li>Hybrid Cloud storage - Application hosting</li>
<li>Media hosting</li>
<li>Data lakes & big data analytics - Software delivery</li>
<li>Static website</li>
</ul>
<h1>S3 - Buckets</h1>

<ul><li>S3 allows people to store objects(files) in "buckets" (directories)</li>
<li>Buckets MUST have a globally unique name (across all regions all accounts)</li>
<li>buckets are defined at the region level</li>
<li>S3 looks like a global service, but buckets are created in a region</li>
<li>Naming convention for buckets:</li>
</ul>  -No uppercase, No underscore
  -3-63 characters long
  -Not an IP address
  -Must start with lowercase letter or number
  -Must NOT start with the prefix <b>xn--</b>
  -Must NOT end with the suffix <b>-s3alias</b>
<ul><li>buckets have encryption configured by default, objects are automatically encrypted</li>
</ul>
<h1>S3 - Objects</h1>

<ul><li>objects are a file that have a key</li>
<li>Objects(files) have a Key</li>
<li>the key: is the FULL path:</li>
</ul>  -s3://my-bucket/my_file.txt
  -OR (if it is in a bucket)
  -s3://my-bucket/my<i>folder1/another</i>_folder/my_file.txt
<ul><li>the key is composed of a prefix + object name</li>
</ul>  -Ex:
  -s3://my-bucket/my<i>file.txt => s3://my-bucket/my</i>_folder1/ (prefix) + my_file.txt(object)
<ul><li>There's no concept of "directories" within buckets(but the UI will trick you to think otherwise)</li>
</ul>  -Just keys with long names that contain slashes "/"
<ul><li>Object values are the file content and the metadata</li>
</ul>  -Object values are the content of the body(a file)
  -Max object size is 5TB
  -if uploading more than 5 GB, must use "multi-part upload"
  -Metadata (list of text key / value pairs - system or user metadata)
  -Tags (Unicode key / value pair - up to 10) - useful for security / lifecycle
  -Version ID (if versioning is enabled)

<h1>S3 - Security</h1>

<ul><li>User-Based</li>
</ul>  -- which API calls should be allowed for a specific user from IAM
<ul><li>Resource-Based</li>
</ul>  -Bucket Policies: bucket wide rules from the S3 console -
  -ex: can allows cross account access(allowing a user from a different account to view our buckets)
  -Object Access Control List(ACL): finer grain (can be disabled)
  -Bucket Access Control List (ACL): less common(can be disabled)
<ul><li>Note: an IAM principal can access an S3 object if:</li>
</ul>  -the user's IAM permissions Allow it OR the resource policy allows it AND there's no explicit deny
<ul><li>Encryption: encrypts objects in Amazon S3 using encryption keys</li>
</ul>
<h1>S3 Bucket policies</h1>

<ul><li>JSON based policies - See: for something similar</li>
</ul>  -Resources: buckets and objects(what to apply this policy to)
  -Effect: Allow or Deny
  -Actions: Set of API actions to allow or deny (what actions to allow or deny)
  -Principal: Account or user to apply the policy to (\* = all users and accounts)
<ul><li>Use S3 bucket for policy to:</li>
</ul>  -Grant public access to the bucket
  -Force objects to the encrypted at upload
  -Grant access to another account(Cross account)

<h3>Example situation: Public Access - Use Bucket policy</h3>

<ul><li>an anonymous visitor wants to access your bucket</li>
<li>once the bucket policy has been attached, the visitor can access the bucket</li>
</ul>
<h3>Example: User Access to S3 - IAM permissions</h3>

<ul><li>giving access to an IAM user</li>
<li>we assign an IAM policy to the IAM user. then they can access the S3 bucket</li>
</ul>
<h3>Example: EC2 Instance access - Use IAM roles</h3>

<ul><li>To allow an EC2 instance to read an S3 bucket:</li>
<li>Create an IAM role (an EC2 instance role in this case) to give permission to the EC2 instance</li>
</ul>
<h3>Advanced: Cross-account access - use bucket policy</h3>

<ul><li>if we want to allow a cross-account access from an IAM user:</li>
<li>Create an S3 bucket policy that allows cross-account access</li>
</ul>
<h3>Bucket settings for Block Public access</h3>

<ul><li>These settings were created to prevent company data leaks</li>
<li>if you know your bucket should never be public, leave these on</li>
<li>Can be set at the account level</li>
</ul>
<h1>S3 - Static Website Hosting</h1>

<ul><li>S3 can host static websites and have them accessible on the internet</li>
<li>The website URL will be:</li>
</ul>  -\http://bucket-name.s3-website-aws-region.amazonaws.com OR
  -\http://bucket-name.s3-website.aws-region.amazonaws.com
<ul><li>need to have public reads available for this to work</li>
<li>If you get a 403 forbidden error: make sure the bucket policy allows public reads</li>
</ul>
<h1>S3 - Versioning</h1>

<ul><li>you can version your files in Amazon S3</li>
<li>it is enabled at the bucket level</li>
<li>Same key overwrite will change the "version": 1,2,3</li>
<li>It is best practice to version your buckets</li>
</ul>  -why?:
  -Protect against unintended deletes(ability to restore a version)
  -Fast roll back to previous versions
  -
<ul><li>Notes</li>
</ul>  -any file that is not versioned prior to enabling versioning will have version "null"
  -suspending version does NOT delete the previous versions

<h1>S3 - Replication(CRR & SRR)</h1>

<ul><li>Must <b>enable Versioning</b> in source and destination buckets</li>
<li>## Goal: async replication of data from bucket to another</li>
<li><b>Cross-Region Replication: </b>source and destination buckets are in different regions</li>
<li><b>Same-Region Replication</b> source and destination buckets are in different regions</li>
<li>Buckets <b>can be in different AWS accounts</b></li>
<li>Copying is asynchronous</li>
<li>Must give proper IAM permissions to S3</li>
<li>Use Cases:</li>
</ul>  -CRR - compliance, lower latency access, replication across accounts
  -SRR - log aggregation, live replication between production and test accounts

<h1>S3 Durability and Availability</h1>

<ul><li>Durability</li>
</ul>  -High durability(Eleven 9s) of objects across multiple AZs
  -Same for all storage classes
  -If you store 10,000,000 objects with Amazon S3, you can on average expect to incur a loss of a single object once every 10,000 years
<ul><li>Availability</li>
</ul>  -Measures how readily available a service is
  -Varies depending on storage class
  -Ex: S3 standard has 99.99% availability = not available 53 minutes a year

<h1>S3 Storage Classes</h1>

All Storage classes:

<ul><li>Amazon S3 Standard - General Purpose</li>
<li>Amazon S3 Standard-Infrequent access (IA)</li>
<li>Amazon S3 One Zone-Infrequent access</li>
<li>Amazon S3 Glacier Instant Retrieval</li>
<li>Amazon S3 Glacier Flexible Retrieval</li>
<li>Amazon S3 Glacier Deep Archive</li>
<li>Amazon S3 Intelligent Tiering</li>
<li>Can move between classes manually or using S3 Lifecycle configurations</li>
</ul>
<h2>S3 Standard - General Purpose</h2>

<ul><li>Used for: frequently accessed data</li>
<li>low latency, high throughput</li>
<li>use cases: big data analytics, mobile & gaming applications, content distribution, etc</li>
</ul>
<h2>S3 Storage Classe: Infrequent Access</h2>

<ul><li>Used for: data that is less frequently accessed, but requires rapid access when needed</li>
<li>Cheaper than S3 standard</li>
<li>Amazon S3 Standard-Infrequent Access (S3 Standard-IA)</li>
<li>4 9s availability</li>
<li>Use cases: Disaster recovery, backups</li>
<li>Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)</li>
<li>High durability(Eleven 9s) in a single AZ, data lost when AZ is destroyed</li>
<li>99.5% availability</li>
<li>Use cases: storing secondray backup copies of on-premise data, or data you can recreate</li>
</ul>
<h2>Amazon S3 Glacier Storage Classes</h2>

<ul><li>Low cost object storage</li>
<li>Used for: archiving/backup</li>
<li>Pricing: price for storage + object retrieval</li>
<li>Amazon S3 Glacier Instant Retrieval</li>
<li>Millisecond retrieval, great for data accessed once a quarter * Minimum storage duration of 90 days</li>
<li>Amazon S3 Glacier Flexible Retrieval (formerly Amazon S3 Glacier):</li>
<li>Expedited (1 to 5 minutes), Standard (3 to 5 hours), Bulk (5 to 12 hours) - free</li>
<li>Minimum storage duration of 90 days</li>
<li>Amazon S3 Glacier Deep Archive - for long term storage:</li>
<li>Standard (12 hours), Bulk (48 hours)</li>
<li>Minimum storage duration of 180 days</li>
</ul>
<h2>S3 Intelligent-Tiering</h2>

<ul><li>function: Moves objects automatically between access tiers based on usage</li>
<li>No retrieval charges in S3 Intelligent-Tiering</li>
<li>Small monthly monitoring and auto-tiering fee</li>
<li>Frequent Access tier (automatic): default tier</li>
<li>Infrequent Access tier (automatic): objects not accessed for 30 days</li>
<li>Archive Instant Access tier (automatic): objects not accessed for 90 days</li>
<li>Archive Access tier (optional): configurable from 90 days to 700+ days</li>
<li>Deep Archive Access tier (optional): config. from 180 days to 700+ days</li>
</ul>
<h2>S3 Storage Classes Comparison</h2>

<h2>S3 Storage classes - Price Comparison example(us-east-1)</h2>

<h2>S3 Encryption</h2>

<h1>Shared Responsbility Model for S3</h1>

<ul><li>AWS: Infrastructure, Configuration, vulnerability analysis, compliance validation</li>
<li>User: S3 Versioning, S3 Bucket policies, S3 replication setup, Logging and monitoring, S3 storage classes</li>
<li>Data encryption and in transit</li>
</ul>
<h1>aws storage gateway</h1>

<h2>Hybrid Cloud for Storage</h2>

<ul><li>aws is pushing for "hybrid cloud": some infra on-prem and some infra in the cloud</li>
<li>Why hybrid?</li>
<li>Long cloud migrations</li>
<li>Security requirements</li>
<li>Compliance requirements</li>
<li>IT strategy</li>
<li>Problem: S3 is a propietary storage technology(unlike EFS/NFS) so how do you expose the S3 data on-premise?</li>
<li>Solution: aws storage gateway</li>
</ul>
<h2>aws storage gateway</h2>

<ul><li>Bridge between on-premise data and cloud-data in S3</li>
<li>Hybrid storage service to allow on-premises to seamlessly use the AWS Cloud</li>
<li>defn: hybrid solution to extend on-premises storage to S3</li>
<li>Use cases: disaster recovery, backup & restore, tiered storage</li>
<li>Don't need to know types of storage gateway</li>
</ul>
<h2>AWS Storage Cloud-Native Options</h2>

<h1>S3 - Summary</h1>

<ul><li>Buckets v. Objects</li>
<li>S3 security</li>
<li>S3 websites</li>
<li>S3 versioning</li>
<li>S3 replication</li>
<li>S3 replication</li>
<li>Snow Family</li>
<li>OpsHub</li>
<li>Storage Gateway</li>
</ul>
Intro

<ul><li>One of the main buildings blocks of aws</li>
<li>advertised as "infinitely scaling" storage</li>
<li>Many websites use amazon s3 as a backbone</li>
<li>many asws services use s3 as an integration as well</li>
</ul>
<h1>Amazon S3 Use cases</h1>

<ul><li>Backup and storage</li>
<li>Disaster Recovery</li>
<li>Archive</li>
<li>Hybrid Cloud storage - Application hosting</li>
<li>Media hosting</li>
<li>Data lakes & big data analytics - Software delivery</li>
<li>Static website</li>
</ul>
<h1>S3 - Buckets</h1>

<ul><li>S3 allows people to store objects(files) in "buckets" (directories)</li>
<li>Buckets MUST have a globally unique name (across all regions all accounts)</li>
<li>buckets are defined at the region level</li>
<li>S3 looks like a global service, but buckets are created in a region</li>
<li>Naming convention for buckets:</li>
<li>No uppercase, No underscore</li>
<li>3-63 characters long</li>
<li>Not an IP address</li>
<li>Must start with lowercase letter or number</li>
<li>Must NOT start with the prefix <b>xn--</b></li>
<li>Must NOT end with the suffix <b>-s3alias</b></li>
<li>buckets have encryption configured by default, objects are automatically encrypted</li>
</ul>
<h1>S3 - Objects</h1>

<ul><li>objects are a file that have a key</li>
<li>Objects(files) have a Key</li>
<li>the key: is the FULL path:</li>
<li>s3://my-bucket/my_file.txt</li>
<li>OR (if it is in a bucket)</li>
<li>s3://my-bucket/my<i>folder1/another</i>_folder/my_file.txt</li>
<li>the key is composed of a prefix + object name</li>
<li>Ex:</li>
<li>s3://my-bucket/my<i>file.txt => s3://my-bucket/my</i>_folder1/ (prefix) + my_file.txt(object)</li>
<li>There's no concept of "directories" within buckets(but the UI will trick you to think otherwise)</li>
</ul>  -Just keys with long names that contain slashes "/"
<ul><li>Object values are the file content and the metadata</li>
</ul>  -Object values are the content of the body(a file)
  -Max object size is 5TB
  -if uploading more than 5 GB, must use "multi-part upload"
  -Metadata (list of text key / value pairs - system or user metadata)
  -Tags (Unicode key / value pair - up to 10) - useful for security / lifecycle
  -Version ID (if versioning is enabled)

<h1>S3 - Security</h1>

<ul><li>User-Based</li>
</ul>  -- which API calls should be allowed for a specific user from IAM
<ul><li>Resource-Based</li>
</ul>  -Bucket Policies: bucket wide rules from the S3 console -
  -ex: can allows cross account access(allowing a user from a different account to view our buckets)
  -Object Access Control List(ACL): finer grain (can be disabled)
  -Bucket Access Control List (ACL): less common(can be disabled)
<ul><li>Note: an IAM principal can access an S3 object if:</li>
</ul>  -the user's IAM permissions Allow it OR the resource policy allows it AND there's no explicit deny
<ul><li>Encryption: encrypts objects in Amazon S3 using encryption keys</li>
</ul>
<h1>S3 Bucket policies</h1>

<ul><li>JSON based policies - See: for something similar</li>
</ul>  -Resources: buckets and objects(what to apply this policy to)
  -Effect: Allow or Deny
  -Actions: Set of API actions to allow or deny (what actions to allow or deny)
  -Principal: Account or user to apply the policy to (\* = all users and accounts)
<ul><li>Use S3 bucket for policy to:</li>
</ul>  -Grant public access to the bucket
  -Force objects to the encrypted at upload
  -Grant access to another account(Cross account)

<h3>Example situation: Public Access - Use Bucket policy</h3>

<ul><li>an anonymous visitor wants to access your bucket</li>
<li>once the bucket policy has been attached, the visitor can access the bucket</li>
</ul>
<h3>Example: User Access to S3 - IAM permissions</h3>

<ul><li>giving access to an IAM user</li>
<li>we assign an IAM policy to the IAM user. then they can access the S3 bucket</li>
</ul>
<h3>Example: EC2 Instance access - Use IAM roles</h3>

<ul><li>To allow an EC2 instance to read an S3 bucket:</li>
<li>Create an IAM role (an EC2 instance role in this case) to give permission to the EC2 instance</li>
</ul>
<h3>Advanced: Cross-account access - use bucket policy</h3>

<ul><li>if we want to allow a cross-account access from an IAM user:</li>
<li>Create an S3 bucket policy that allows cross-account access</li>
</ul>
<h3>Bucket settings for Block Public access</h3>

<ul><li>These settings were created to prevent company data leaks</li>
<li>if you know your bucket should never be public, leave these on</li>
<li>Can be set at the account level</li>
</ul>
<h1>S3 - Static Website Hosting</h1>

<ul><li>S3 can host static websites and have them accessible on the internet</li>
<li>The website URL will be:</li>
</ul>  -\http://bucket-name.s3-website-aws-region.amazonaws.com OR
  -\http://bucket-name.s3-website.aws-region.amazonaws.com
<ul><li>need to have public reads available for this to work</li>
<li>If you get a 403 forbidden error: make sure the bucket policy allows public reads</li>
</ul>
<h1>S3 - Versioning</h1>

<ul><li>you can version your files in Amazon S3</li>
<li>it is enabled at the bucket level</li>
<li>Same key overwrite will change the "version": 1,2,3</li>
<li>It is best practice to version your buckets</li>
</ul>  -why?:
  -Protect against unintended deletes(ability to restore a version)
  -Fast roll back to previous versions
  -
<ul><li>Notes</li>
</ul>  -any file that is not versioned prior to enabling versioning will have version "null"
  -suspending version does NOT delete the previous versions

<h1>S3 - Replication(CRR & SRR)</h1>

<ul><li>Must <b>enable Versioning</b> in source and destination buckets</li>
<li>## Goal: async replication of data from bucket to another</li>
<li><b>Cross-Region Replication: </b>source and destination buckets are in different regions</li>
<li><b>Same-Region Replication</b> source and destination buckets are in different regions</li>
<li>Buckets <b>can be in different AWS accounts</b></li>
<li>Copying is asynchronous</li>
<li>Must give proper IAM permissions to S3</li>
<li>Use Cases:</li>
</ul>  -CRR - compliance, lower latency access, replication across accounts
  -SRR - log aggregation, live replication between production and test accounts

<h1>S3 Durability and Availability</h1>

<ul><li>Durability</li>
</ul>  -High durability(Eleven 9s) of objects across multiple AZs
  -Same for all storage classes
  -If you store 10,000,000 objects with Amazon S3, you can on average expect to incur a loss of a single object once every 10,000 years
<ul><li>Availability</li>
</ul>  -Measures how readily available a service is
  -Varies depending on storage class
  -Ex: S3 standard has 99.99% availability = not available 53 minutes a year

<h1>S3 Storage Classes</h1>

All Storage classes:

<ul><li>Amazon S3 Standard - General Purpose</li>
<li>Amazon S3 Standard-Infrequent access (IA)</li>
<li>Amazon S3 One Zone-Infrequent access</li>
<li>Amazon S3 Glacier Instant Retrieval</li>
<li>Amazon S3 Glacier Flexible Retrieval</li>
<li>Amazon S3 Glacier Deep Archive</li>
<li>Amazon S3 Intelligent Tiering</li>
<li>Can move between classes manually or using S3 Lifecycle configurations</li>
</ul>
<h2>S3 Standard - General Purpose</h2>

<ul><li>Used for: frequently accessed data</li>
<li>low latency, high throughput</li>
<li>use cases: big data analytics, mobile & gaming applications, content distribution, etc</li>
</ul>
<h2>S3 Storage Classe: Infrequent Access</h2>

<ul><li>Used for: data that is less frequently accessed, but requires rapid access when needed</li>
<li>Cheaper than S3 standard</li>
<li>Amazon S3 Standard-Infrequent Access (S3 Standard-IA)</li>
</ul>  -4 9s availability
  -Use cases: Disaster recovery, backups
<ul><li>Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)</li>
</ul>  -High durability(Eleven 9s) in a single AZ, data lost when AZ is destroyed
  -99.5% availability
  -Use cases: storing secondray backup copies of on-premise data, or data you can recreate

<h2>Amazon S3 Glacier Storage Classes</h2>

<ul><li>Low cost object storage</li>
<li>Used for: archiving/backup</li>
<li>Pricing: price for storage + object retrieval</li>
<li>Amazon S3 Glacier Instant Retrieval</li>
</ul>  -Millisecond retrieval, great for data accessed once a quarter * Minimum storage duration of 90 days
<ul><li>Amazon S3 Glacier Flexible Retrieval (formerly Amazon S3 Glacier):</li>
</ul>  -Expedited (1 to 5 minutes), Standard (3 to 5 hours), Bulk (5 to 12 hours) - free
  -Minimum storage duration of 90 days
<ul><li>Amazon S3 Glacier Deep Archive - for long term storage:</li>
</ul>  -Standard (12 hours), Bulk (48 hours)
  -Minimum storage duration of 180 days

<h2>S3 Intelligent-Tiering</h2>

<ul><li>function: Moves objects automatically between access tiers based on usage</li>
<li>No retrieval charges in S3 Intelligent-Tiering</li>
<li>Small monthly monitoring and auto-tiering fee</li>
<li>Frequent Access tier (automatic): default tier</li>
<li>Infrequent Access tier (automatic): objects not accessed for 30 days</li>
<li>Archive Instant Access tier (automatic): objects not accessed for 90 days</li>
<li>Archive Access tier (optional): configurable from 90 days to 700+ days</li>
<li>Deep Archive Access tier (optional): config. from 180 days to 700+ days</li>
</ul>
<h2>S3 Storage Classes Comparison</h2>

<h2>S3 Storage classes - Price Comparison example(us-east-1)</h2>

<h2>S3 Encryption</h2>

<h1>Shared Responsbility Model for S3</h1>

<ul><li>AWS: Infrastructure, Configuration, vulnerability analysis, compliance validation</li>
<li>User: S3 Versioning, S3 Bucket policies, S3 replication setup, Logging and monitoring, S3 storage classes</li>
<li>Data encryption and in transit</li>
</ul>
<h1>aws storage gateway</h1>

<h2>Hybrid Cloud for Storage</h2>

<ul><li>aws is pushing for "hybrid cloud": some infra on-prem and some infra in the cloud</li>
<li>Why hybrid?</li>
</ul>  -Long cloud migrations
  -Security requirements
  -Compliance requirements
  -IT strategy
<ul><li>Problem: S3 is a propietary storage technology(unlike EFS/NFS) so how do you expose the S3 data on-premise?</li>
<li>Solution: aws storage gateway</li>
</ul>
<h2>aws storage gateway</h2>

<ul><li>Bridge between on-premise data and cloud-data in S3</li>
<li>Hybrid storage service to allow on-premises to seamlessly use the AWS Cloud</li>
<li>defn: hybrid solution to extend on-premises storage to S3</li>
<li>Use cases: disaster recovery, backup & restore, tiered storage</li>
<li>Don't need to know types of storage gateway</li>
</ul>
<h2>AWS Storage Cloud-Native Options</h2>

<h1>S3 - Summary</h1>

<ul><li>Buckets v. Objects</li>
<li>S3 security</li>
<li>S3 websites</li>
<li>S3 versioning</li>
<li>S3 replication</li>
<li>S3 replication</li>
<li>Snow Family</li>
<li>OpsHub</li>
<li>Storage Gateway</li>
</ul>
Intro

<ul><li>One of the main buildings blocks of aws</li>
<li>advertised as "infinitely scaling" storage</li>
<li>Many websites use amazon s3 as a backbone</li>
<li>many asws services use s3 as an integration as well</li>
</ul>
<h1>Amazon S3 Use cases</h1>

<ul><li>Backup and storage</li>
<li>Disaster Recovery</li>
<li>Archive</li>
<li>Hybrid Cloud storage - Application hosting</li>
<li>Media hosting</li>
<li>Data lakes & big data analytics - Software delivery</li>
<li>Static website</li>
</ul>
<h1>S3 - Buckets</h1>

<ul><li>S3 allows people to store objects(files) in "buckets" (directories)</li>
<li>Buckets MUST have a globally unique name (across all regions all accounts)</li>
<li>buckets are defined at the region level</li>
<li>S3 looks like a global service, but buckets are created in a region</li>
<li>Naming convention for buckets:</li>
</ul>  -No uppercase, No underscore
  -3-63 characters long
  -Not an IP address
  -Must start with lowercase letter or number
  -Must NOT start with the prefix <b>xn--</b>
  -Must NOT end with the suffix <b>-s3alias</b>
<ul><li>buckets have encryption configured by default, objects are automatically encrypted</li>
</ul>
<h1>S3 - Objects</h1>

<ul><li>objects are a file that have a key</li>
<li>Objects(files) have a Key</li>
<li>the key: is the FULL path:</li>
</ul>  -s3://my-bucket/my_file.txt
  -OR (if it is in a bucket)
  -s3://my-bucket/my<i>folder1/another</i>_folder/my_file.txt
<ul><li>the key is composed of a prefix + object name</li>
</ul>  -Ex:
  -s3://my-bucket/my<i>file.txt => s3://my-bucket/my</i>_folder1/ (prefix) + my_file.txt(object)
<ul><li>There's no concept of "directories" within buckets(but the UI will trick you to think otherwise)</li>
</ul>  -Just keys with long names that contain slashes "/"
<ul><li>Object values are the file content and the metadata</li>
</ul>  -Object values are the content of the body(a file)
  -Max object size is 5TB
  -if uploading more than 5 GB, must use "multi-part upload"
  -Metadata (list of text key / value pairs - system or user metadata)
  -Tags (Unicode key / value pair - up to 10) - useful for security / lifecycle
  -Version ID (if versioning is enabled)

<h1>S3 - Security</h1>

<ul><li>User-Based</li>
</ul>  -- which API calls should be allowed for a specific user from IAM
<ul><li>Resource-Based</li>
</ul>  -Bucket Policies: bucket wide rules from the S3 console -
  -ex: can allows cross account access(allowing a user from a different account to view our buckets)
  -Object Access Control List(ACL): finer grain (can be disabled)
  -Bucket Access Control List (ACL): less common(can be disabled)
<ul><li>Note: an IAM principal can access an S3 object if:</li>
</ul>  -the user's IAM permissions Allow it OR the resource policy allows it AND there's no explicit deny
<ul><li>Encryption: encrypts objects in Amazon S3 using encryption keys</li>
</ul>
<h1>S3 Bucket policies</h1>

<ul><li>JSON based policies - See: for something similar</li>
</ul>  -Resources: buckets and objects(what to apply this policy to)
  -Effect: Allow or Deny
  -Actions: Set of API actions to allow or deny (what actions to allow or deny)
  -Principal: Account or user to apply the policy to (\* = all users and accounts)
<ul><li>Use S3 bucket for policy to:</li>
</ul>  -Grant public access to the bucket
  -Force objects to the encrypted at upload
  -Grant access to another account(Cross account)

<h3>Example situation: Public Access - Use Bucket policy</h3>

<ul><li>an anonymous visitor wants to access your bucket</li>
<li>once the bucket policy has been attached, the visitor can access the bucket</li>
</ul>
<h3>Example: User Access to S3 - IAM permissions</h3>

<ul><li>giving access to an IAM user</li>
<li>we assign an IAM policy to the IAM user. then they can access the S3 bucket</li>
</ul>
<h3>Example: EC2 Instance access - Use IAM roles</h3>

<ul><li>To allow an EC2 instance to read an S3 bucket:</li>
<li>Create an IAM role (an EC2 instance role in this case) to give permission to the EC2 instance</li>
</ul>
<h3>Advanced: Cross-account access - use bucket policy</h3>

<ul><li>if we want to allow a cross-account access from an IAM user:</li>
<li>Create an S3 bucket policy that allows cross-account access</li>
</ul>
<h3>Bucket settings for Block Public access</h3>

<ul><li>These settings were created to prevent company data leaks</li>
<li>if you know your bucket should never be public, leave these on</li>
<li>Can be set at the account level</li>
</ul>
<h1>S3 - Static Website Hosting</h1>

<ul><li>S3 can host static websites and have them accessible on the internet</li>
<li>The website URL will be:</li>
</ul>  -\http://bucket-name.s3-website-aws-region.amazonaws.com OR
  -\http://bucket-name.s3-website.aws-region.amazonaws.com
<ul><li>need to have public reads available for this to work</li>
<li>If you get a 403 forbidden error: make sure the bucket policy allows public reads</li>
</ul>
<h1>S3 - Versioning</h1>

<ul><li>you can version your files in Amazon S3</li>
<li>it is enabled at the bucket level</li>
<li>Same key overwrite will change the "version": 1,2,3</li>
<li>It is best practice to version your buckets</li>
</ul>  - why?:
  - Protect against unintended deletes(ability to restore a version)
  - Fast roll back to previous versions
  -
<ul><li>Notes</li>
</ul>  -any file that is not versioned prior to enabling versioning will have version "null"
  -suspending version does NOT delete the previous versions

<h1>S3 - Replication(CRR & SRR)</h1>

<ul><li>Must <b>enable Versioning</b> in source and destination buckets</li>
<li>## Goal: async replication of data from bucket to another</li>
<li><b>Cross-Region Replication: </b>source and destination buckets are in different regions</li>
<li><b>Same-Region Replication</b> source and destination buckets are in different regions</li>
<li>Buckets <b>can be in different AWS accounts</b></li>
<li>Copying is asynchronous</li>
<li>Must give proper IAM permissions to S3</li>
<li>Use Cases:</li>
</ul>  -CRR - compliance, lower latency access, replication across accounts
  -SRR - log aggregation, live replication between production and test accounts

<h1>S3 Durability and Availability</h1>

<ul><li>Durability</li>
</ul>  -High durability(Eleven 9s) of objects across multiple AZs
  -Same for all storage classes
  -If you store 10,000,000 objects with Amazon S3, you can on average expect to incur a loss of a single object once every 10,000 years
<ul><li>Availability</li>
</ul>  -Measures how readily available a service is
  -Varies depending on storage class
  -Ex: S3 standard has 99.99% availability = not available 53 minutes a year

<h1>S3 Storage Classes</h1>

All Storage classes:

<ul><li>Amazon S3 Standard - General Purpose</li>
<li>Amazon S3 Standard-Infrequent access (IA)</li>
<li>Amazon S3 One Zone-Infrequent access</li>
<li>Amazon S3 Glacier Instant Retrieval</li>
<li>Amazon S3 Glacier Flexible Retrieval</li>
<li>Amazon S3 Glacier Deep Archive</li>
<li>Amazon S3 Intelligent Tiering</li>
<li>Can move between classes manually or using S3 Lifecycle configurations</li>
</ul>
<h2>S3 Standard - General Purpose</h2>

<ul><li>Used for: frequently accessed data</li>
<li>low latency, high throughput</li>
<li>use cases: big data analytics, mobile & gaming applications, content distribution, etc</li>
</ul>
<h2>S3 Storage Classe: Infrequent Access</h2>

<ul><li>Used for: data that is less frequently accessed, but requires rapid access when needed</li>
<li>Cheaper than S3 standard</li>
<li>Amazon S3 Standard-Infrequent Access (S3 Standard-IA)</li>
</ul>  -4 9s availability
  -Use cases: Disaster recovery, backups
<ul><li>Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)</li>
</ul>  -High durability(Eleven 9s) in a single AZ, data lost when AZ is destroyed
  -99.5% availability
  -Use cases: storing secondray backup copies of on-premise data, or data you can recreate

<h2>Amazon S3 Glacier Storage Classes</h2>

<ul><li>Low cost object storage</li>
<li>Used for: archiving/backup</li>
<li>Pricing: price for storage + object retrieval</li>
<li>Amazon S3 Glacier Instant Retrieval</li>
</ul>  -Millisecond retrieval, great for data accessed once a quarter * Minimum storage duration of 90 days
<ul><li>Amazon S3 Glacier Flexible Retrieval (formerly Amazon S3 Glacier):</li>
</ul>  -Expedited (1 to 5 minutes), Standard (3 to 5 hours), Bulk (5 to 12 hours) - free
  -Minimum storage duration of 90 days
<ul><li>Amazon S3 Glacier Deep Archive - for long term storage:</li>
</ul>  -Standard (12 hours), Bulk (48 hours)
  -Minimum storage duration of 180 days

<h2>S3 Intelligent-Tiering</h2>

<ul><li>function: Moves objects automatically between access tiers based on usage</li>
<li>No retrieval charges in S3 Intelligent-Tiering</li>
<li>Small monthly monitoring and auto-tiering fee</li>
<li>Frequent Access tier (automatic): default tier</li>
<li>Infrequent Access tier (automatic): objects not accessed for 30 days</li>
<li>Archive Instant Access tier (automatic): objects not accessed for 90 days</li>
<li>Archive Access tier (optional): configurable from 90 days to 700+ days</li>
<li>Deep Archive Access tier (optional): config. from 180 days to 700+ days</li>
</ul>
<h2>S3 Storage Classes Comparison</h2>

<h2>S3 Storage classes - Price Comparison example(us-east-1)</h2>

<h2>S3 Encryption</h2>

<h1>Shared Responsbility Model for S3</h1>

<ul><li>AWS: Infrastructure, Configuration, vulnerability analysis, compliance validation</li>
<li>User: S3 Versioning, S3 Bucket policies, S3 replication setup, Logging and monitoring, S3 storage classes</li>
<li>Data encryption and in transit</li>
</ul>
<h1>aws storage gateway</h1>

<h2>Hybrid Cloud for Storage</h2>

<ul><li>aws is pushing for "hybrid cloud": some infra on-prem and some infra in the cloud</li>
<li>Why hybrid?</li>
</ul>  -Long cloud migrations
  -Security requirements
  -Compliance requirements
  -IT strategy
<ul><li>Problem: S3 is a propietary storage technology(unlike EFS/NFS) so how do you expose the S3 data on-premise?</li>
<li>Solution: aws storage gateway</li>
</ul>
<h2>aws storage gateway</h2>

<ul><li>Bridge between on-premise data and cloud-data in S3</li>
<li>Hybrid storage service to allow on-premises to seamlessly use the AWS Cloud</li>
<li>defn: hybrid solution to extend on-premises storage to S3</li>
<li>Use cases: disaster recovery, backup & restore, tiered storage</li>
<li>Don't need to know types of storage gateway</li>
</ul>
<h2>AWS Storage Cloud-Native Options</h2>

<h1>S3 - Summary</h1>

<ul><li>Buckets v. Objects</li>
<li>S3 security</li>
<li>S3 websites</li>
<li>S3 versioning</li>
<li>S3 replication</li>
<li>S3 replication</li>
<li>Snow Family</li>
<li>OpsHub</li>
<li>Storage Gateway</li>
</ul>